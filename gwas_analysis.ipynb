{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set path to where `mlruns` directory is located (usually, the `CardiacCOMA` repository)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CARDIAC_COMA_REPO = \"/home/rodrigo/01_repos/CardiacCOMA/\"\n",
    "CARDIAC_GWAS_REPO = \"/home/rodrigo/01_repos/CardiacGWAS/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import os, sys\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os; os.chdir(CARDIAC_COMA_REPO)\n",
    "from config.load_config import load_yaml_config, to_dict\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from IPython.display import Image\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "import pickle as pkl\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from argparse import Namespace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import surgeon_pytorch\n",
    "#from surgeon_pytorch import Inspect, get_layers\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython import embed\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import model.Model3D\n",
    "from utils.helpers import get_coma_args, get_lightning_module, get_datamodule\n",
    "from copy import deepcopy\n",
    "from pprint import pprint\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from IPython import embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow_helpers import \\\n",
    "    list_artifacts,\\\n",
    "    get_significant_loci,\\\n",
    "    get_metrics_cols, \\\n",
    "    get_params_cols, \\\n",
    "    get_runs_df, \\\n",
    "    get_good_runs,\\\n",
    "    summarize_loci_across_runs,\\\n",
    "    get_model_pretrained_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRACKING_URI = f\"file://{CARDIAC_COMA_REPO}/mlruns\"\n",
    "mlflow.set_tracking_uri(TRACKING_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUNS_CACHED = \"../CardiacGWAS/results/runs.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select MLflow experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment_selection_widget():\n",
    "    \n",
    "    options = [exp.name for exp in mlflow.list_experiments()]\n",
    "\n",
    "    experiment_w = widgets.Select(\n",
    "      options=options,\n",
    "      value=options[1],\n",
    "      description=\"Select MLflow experiment\"\n",
    "    )\n",
    "    \n",
    "    return experiment_w\n",
    "\n",
    "exp_w = experiment_selection_widget()\n",
    "\n",
    "@interact\n",
    "def get_runs(exp_name=exp_w):  \n",
    "  #try:\n",
    "    global exp_id, runs_df\n",
    "    exp_id = mlflow.get_experiment_by_name(exp_name).experiment_id\n",
    "    _get_runs_df = partial(get_runs_df, sort_by=None)\n",
    "    runs_df = _get_runs_df(exp_name=exp_name, only_finished=True)\n",
    "    metrics, params = get_metrics_cols(runs_df), get_params_cols(runs_df)  \n",
    "    # display(runs_df.loc[:, [*metrics, *params]].drop(\"params.platform\", axis=1).head(10))    \n",
    "  #except:\n",
    "#    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve run data from MLflow for the chosen experiment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_name = \"Cardiac - ED\"\n",
    "if not os.path.exists(RUNS_CACHED):\n",
    "    exp_id = mlflow.get_experiment_by_name(exp_name).experiment_id\n",
    "    _get_runs_df = partial(get_runs_df, sort_by=None)\n",
    "    runs_df = _get_runs_df(exp_name=exp_name, only_finished=True)\n",
    "    metrics, params = get_metrics_cols(runs_df), get_params_cols(runs_df)\n",
    "    runs_df.to_csv(RUNS_CACHED)\n",
    "else:\n",
    "    runs_df = pd.read_csv(RUNS_CACHED)\n",
    "    runs_df = runs_df.set_index([\"experiment_id\", \"run_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve `run_id`'s where average MSE is less than a threshold (in mm$^2$) for the test dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RECON_LOSS_THRES = 2. # performance threshold for MSE mm2.\n",
    "run_ids = sorted([x for x in runs_df[runs_df[\"metrics.test_recon_loss\"] < RECON_LOSS_THRES].index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of runs, $R$, and number of latent variables tested $n_z=\\sum_{r=1}^{R} \\dim(\\textbf{z}_r)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_runs = len(run_ids)\n",
    "n_z_total = runs_df.filter(items=run_ids, axis='index').filter(like=\"param\", axis='columns')[\"params.latent_dim\"].sum()\n",
    "print(f\"Number of runs: {n_runs}\")\n",
    "print(f\"Total number of latent variables: {n_z_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Widget for selecting run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "run_ids_w = widgets.Select(description=\"Choose run:\", options={x[:10]: x for x in run_ids})\n",
    "display(run_ids_w)\n",
    "run_id = run_ids_w.value\n",
    "run_info = runs_df.loc[run_id].to_dict()\n",
    "artifact_uri = run_info[\"artifact_uri\"].replace(\"file://\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign gene names to different regions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loci_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from loci_mapping import LOCUS_TO_REGION, REGION_TO_LOCUS, LOCI_TO_DROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_significant_loci(\n",
    "    runs_df,\n",
    "    experiment_id, run_id, \n",
    "    p_threshold=5e-8, \n",
    "    client=mlflow.tracking.MlflowClient()\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    '''    \n",
    "    Returns a DataFrame with the loci that have a stronger p-value than a given threshold\n",
    "    '''\n",
    "    \n",
    "    def get_phenoname(path):        \n",
    "        filename = os.path.basename(path)\n",
    "        phenoname = filename.split(\"__\")[0]\n",
    "        return phenoname\n",
    "        \n",
    "    run_info = runs_df.loc[(experiment_id, run_id)].to_dict()\n",
    "    artifact_uri = run_info[\"artifact_uri\"].replace(\"file://\", \"\")    \n",
    "           \n",
    "    gwas_dir_summaries = os.path.join(artifact_uri, \"GWAS/summaries\")\n",
    "    # gwas_dir_summaries = os.path.join(artifact_uri, \"GWAS_adj_10PCs/summaries\")\n",
    "    \n",
    "    try:\n",
    "        summaries_fileinfo = [ os.path.join(gwas_dir_summaries, x) for x in  os.listdir(gwas_dir_summaries) ]\n",
    "    except:\n",
    "        summaries_fileinfo = []\n",
    "    \n",
    "    # summaries_fileinfo = client._tracking_client.list_artifacts(run_id, path=\"GWAS_adj_10PCs/summaries\")\n",
    "    # summaries_fileinfo = client._tracking_client.list_artifacts(run_id, path=\"GWAS/summaries\")    \n",
    "        \n",
    "    if len(summaries_fileinfo) == 0:\n",
    "        return pd.DataFrame(columns=[\"run\", \"pheno\", \"region\"])\n",
    "    \n",
    "    # region_summaries = {get_phenoname(x.path): os.path.join(artifact_uri, x.path) for x in summaries_fileinfo}\n",
    "    region_summaries = {get_phenoname(x): os.path.join(artifact_uri, x) for x in summaries_fileinfo}\n",
    "    dfs = [pd.read_csv(path).assign(pheno=pheno) for pheno, path in region_summaries.items()]\n",
    "    \n",
    "    df = pd.concat(dfs)\n",
    "    df['locus_name'] = df.apply(lambda row: REGION_TO_LOCUS.get(row[\"region\"], \"Unnamed\"), axis=1)\n",
    "    df = df.set_index([\"pheno\", \"region\"])    \n",
    "    \n",
    "    df_filtered = df[df.P < p_threshold]\n",
    "    \n",
    "    return df_filtered.sort_values(by=\"P\")\n",
    "\n",
    "\n",
    "def summarize_loci_across_runs(runs_df: pd.DataFrame):\n",
    "\n",
    "    '''\n",
    "    Parameters: run_ids\n",
    "    Return: pd.DataFrame with .\n",
    "    '''\n",
    "\n",
    "    # run_ids = sorted([x[1] for x in runs_df[runs_df[\"metrics.test_recon_loss\"] < RECON_LOSS_THRES].index])\n",
    "    run_ids = sorted([x[1] for x in runs_df.index])\n",
    "\n",
    "    all_signif_loci = []\n",
    "    \n",
    "    for run_id in tqdm(run_ids):\n",
    "        signif_loci_df = \\\n",
    "            get_significant_loci(runs_df, experiment_id=1, run_id=run_id).\\\n",
    "            assign(run=run_id).\\\n",
    "            reset_index().\\\n",
    "            set_index([\"run\", \"pheno\", \"region\"]\n",
    "        )                \n",
    "        all_signif_loci.append(signif_loci_df)        \n",
    "      \n",
    "    all_signif_loci = pd.concat(all_signif_loci)    \n",
    "    return all_signif_loci\n",
    "\n",
    "    df = all_signif_loci.\\\n",
    "      groupby([\"region\", \"locus_name\"]).\\\n",
    "      aggregate({\"CHR\":\"count\", \"P\": \"min\"}).\\\n",
    "      rename({\"CHR\":\"count\", \"P\":\"min_P\"}, axis=1).\\\n",
    "      sort_values(\"count\", ascending=False)    \n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ORDER_BY = {\"by\":\"count\", \"ascending\":False}\n",
    "ORDER_BY = {\"by\":\"min_P\", \"ascending\":True}\n",
    "ORDER_BY = {\"by\":\"-log10(min_P)\", \"ascending\":False}\n",
    "\n",
    "all_signif_loci_df = summarize_loci_across_runs(runs_df)\n",
    "\n",
    "good_runs = [x for x in all_signif_loci_df.index if x[0] in [y[1] for y in run_ids]]\n",
    "all_signif_loci_df = all_signif_loci_df.filter(items=good_runs, axis=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signif_loci_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_signif_loci_df = all_signif_loci_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = all_signif_loci_df.assign(best_p_for_region=all_signif_loci_df.groupby(['region']).P.transform(\"min\"))\n",
    "kk = all_signif_loci_df[kk.P == kk.best_p_for_region]\n",
    "kk.BP = kk.BP.astype(int)\n",
    "kk.CHR = kk.CHR.astype(int)\n",
    "kk = kk[~kk.region.isin(LOCI_TO_DROP)]\n",
    "kk = kk.sort_values(\"P\").head(50)\n",
    "\n",
    "COLUMNS = [\"CHR\", \"BP\", \"SNP\", \"region\", \"run\", \"pheno\"]\n",
    "kk = kk[COLUMNS].reset_index(drop=True)\n",
    "kk.to_csv(f\"{CARDIAC_GWAS_REPO}/results/best_z_for_loci.csv\", index=False)\n",
    "kk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_for_threshold = []\n",
    "\n",
    "for exp_id, run_id in run_ids:\n",
    "    \n",
    "    try:\n",
    "        run_df = all_signif_loci_df.loc[run_id]\n",
    "        run_df = run_df.assign(run_id=run_id)\n",
    "    except KeyError:\n",
    "        print(f\"Run {run_id} does not have significant loci.\")\n",
    "        pass\n",
    "    \n",
    "    filter_for_threshold.append(run_df)\n",
    "    \n",
    "filter_for_threshold = pd.concat(filter_for_threshold).reset_index().set_index([\"run_id\", \"pheno\", \"region\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loci_summary_df = all_signif_loci_df.\\\n",
    "      reset_index().\\\n",
    "      drop(\"index\", axis=1).\\\n",
    "      groupby(by=[\"region\", \"locus_name\", \"run\"]).\\\n",
    "      aggregate({\"CHR\":\"count\", \"P\": \"min\"}).\\\n",
    "      rename({\"CHR\":\"count\", \"P\":\"min_P\"}, axis=1).\\\n",
    "      sort_values(\"count\", ascending=False).\\\n",
    "      sort_values(\"min_P\", ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loci_summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the best _p_-value for each genetic locus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loci_grouped_df = loci_summary_df.groupby(level=[\"region\", \"locus_name\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_P_df = loci_grouped_df.\\\n",
    "    min(\"min_P\").\\\n",
    "    drop(\"count\", axis=1).\\\n",
    "    sort_values(\"min_P\")    \n",
    "\n",
    "min_P_df.min_P = [f\"${str(round(float(x[0]), 1))} \\times 10^{{{x[1]}}}$\" for x in min_P_df.min_P.astype(str).str.split(\"e\")]\n",
    "\n",
    "min_P_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loci_counts = loci_grouped_df.\\\n",
    "    count().\\\n",
    "    drop(LOCI_TO_DROP).\\\n",
    "    drop(\"min_P\", axis=1).\\\n",
    "    sort_values('count', ascending=False) # / n_runs * 100\n",
    "\n",
    "loci_counts.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_count_table_tex(tex_file):\n",
    "    \n",
    "    with open(tex_file, \"wt\") as table_f:    \n",
    "        \n",
    "        table_code = pd.merge(loci_counts, min_P_df, left_index=True, right_index=True).\\\n",
    "            reset_index().\\\n",
    "            rename({\"locus_name\": \"locus\", \"min_P\": \"$p$-value\"}, axis=1).\\\n",
    "            to_latex( \n",
    "                escape=False,\n",
    "                index=False\n",
    "            )\n",
    "        \n",
    "        table_code = table_code.replace(\"_\", \"\\_\")\n",
    "        table_f.write()\n",
    "    \n",
    "    return table_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_count_table_tex(f\"{CARDIAC_GWAS_REPO}/manuscript/tables/gwas_counts.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_w=widgets.Select(options=sorted(list(set([x[0] for x in loci_summary_df.index]))))\n",
    "\n",
    "@interact\n",
    "def examine_locus(region=region_w):\n",
    "    display(loci_summary_df.loc[region])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_loci_across_runs(runs_df: pd.DataFrame):\n",
    "\n",
    "   '''\n",
    "   Parameters: run_ids\n",
    "   Return: pd.DataFrame with [\"count\", \"min_P\"].\n",
    "   '''\n",
    "\n",
    "   # run_ids = sorted([x[1] for x in runs_df[runs_df[\"metrics.test_recon_loss\"] < RECON_LOSS_THRES].index])\n",
    "   run_ids = sorted([x[1] for x in runs_df.index])\n",
    "\n",
    "   all_signif_loci = pd.concat([\n",
    "     get_significant_loci(runs_df, \"1\", run).\\\n",
    "       assign(run=run).\\\n",
    "       reset_index().\\\n",
    "       set_index([\"run\", \"pheno\", \"region\"]) \n",
    "     for run in run_ids\n",
    "   ])\n",
    "   \n",
    "   return all_signif_loci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk = summarize_loci_across_runs(runs_df).reset_index().drop(\"index\", axis=1)\n",
    "#kk.pheno = kk.apply(lambda x: f\"1_{x.run[:5]}_{x.pheno}\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_corr = pd.read_csv(\"data/cardio/corr_z_vs_indices.csv\").set_index(\"phenotype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs = []\n",
    "\n",
    "for index, row in pp.sort_values(by=\"region\").iterrows():\n",
    "    try:\n",
    "        corrs.append(list(z_corr.loc[row.pheno]))\n",
    "    except:\n",
    "        corrs.append([pd.NA]*4)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrs_df = pd.DataFrame(corrs, columns=[\"LVEDV_corr\", \"LVM_corr\", \"RVEDV_corr\", \"LVSph_corr\"])\n",
    "corrs_df.set_index(pp.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk_grouped = pd.concat([kk, corrs_df.abs()], axis=1).groupby(\"region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "mean_f = partial(pd.Series.mean, skipna = True)\n",
    "std_f = partial(pd.Series.std, skipna = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = kk_grouped.agg(\"count\")[\"LVEDV_corr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phenos =  [\"LVEDV\", \"LVM\", \"RVEDV\", \"LVSph\"]\n",
    "corr_per_locus = kk_grouped.aggregate(func={f\"{pheno}_corr\": [mean_f, std_f] for pheno in phenos})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_per_locus[\"counts\"] = counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_per_locus.sort_values(by=\"counts\", ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Statistics on the GWAS loci counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signif_loci_dfs = {}\n",
    "dd = []\n",
    "\n",
    "def loci_count(run_df):\n",
    "    from collections import Counter\n",
    "    return dict(Counter([x[1] for x in run_df.index]))\n",
    "\n",
    "for run in runs_df.index:\n",
    "    \n",
    "    try:     \n",
    "      \n",
    "      pp = get_significant_loci(runs_df[runs_df[\"metrics.val_recon_loss\"] < 2], exp_id, run[1]) #.sort_values(by=[\"CHR\", \"BP\"], axis=0)\n",
    "      n_distinct_loci = len(loci_cnt.keys())\n",
    "      n_hits_with_duplication = sum(loci_cnt.values())\n",
    "      \n",
    "      ff = [  run[1], \n",
    "         runs_df.loc[run, \"metrics.test_recon_loss\"], \n",
    "         runs_df.loc[run, \"metrics.test_kld_loss\"], \n",
    "         runs_df.loc[run, \"params.latent_dim\"], \n",
    "         runs_df.loc[run, \"params.w_kl\"],\n",
    "         n_distinct_loci, \n",
    "         n_hits_with_duplication, \n",
    "         n_hits_with_duplication / n_distinct_loci             \n",
    "      ]\n",
    "      \n",
    "      signif_loci_dfs[run[1]] = pp\n",
    "      loci_cnt = loci_count(signif_loci_dfs[run[1]])\n",
    "      dd.append(ff)\n",
    "    except:\n",
    "      pass\n",
    "\n",
    "kk = pd.DataFrame(dd)\n",
    "\n",
    "kk.columns = [\n",
    "    \"run_id\",\n",
    "    \"test_mse\",\n",
    "    \"kld\",    \n",
    "    \"lat_dim\",\n",
    "    \"w_kl\",\n",
    "    \"n_loci\",\n",
    "    \"n_loci_dupl\",\n",
    "    \"ratio\"    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interact(\n",
    "    lambda xcol, ycol: sns.boxplot(x=xcol, y=ycol, data=kk),\n",
    "    xcol = widgets.Select(options=kk.columns),\n",
    "    ycol = widgets.Select(options=kk.columns)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_signif_loci(run_id=run_ids_w):\n",
    "    return get_significant_loci(runs_df, exp_id, run_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
